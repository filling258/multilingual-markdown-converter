{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cb104c-588c-4a0d-b888-6c8449a2bc01",
   "metadata": {},
   "source": [
    "# Tokenizing and Translating Markdown with OpenAI\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will implement a pipeline to tokenize a markdown document, split it into chunks at multiple newlines, translate those chunks using OpenAI's LLMs, and reconstruct the full document while retaining original formatting.\n",
    "\n",
    "Specifically, we will:\n",
    "\n",
    "- Read in a markdown file \n",
    "- Tokenize the text into words, punctuation, etc.\n",
    "- Count the number of tokens\n",
    "- Split the tokens into chunks whenever there are multiple successive newlines \n",
    "- Translate each chunk into another language using OpenAI's translation LLMs\n",
    "- Reconstruct the translated chunks into a full document, preserving original formatting like headers, lists, etc.\n",
    "\n",
    "This allows us to get translations while keeping code blocks, images, tables intact. \n",
    "\n",
    "We'll use Python and Jupyter notebooks to implement the pipeline. The notebook will be structured into sections for each step of the process.\n",
    "\n",
    "To follow along, you'll want a markdown file to process. We'll use a small sample file included with the notebook. You'll need access to OpenAI's API.\n",
    "\n",
    "Let's get started! First we'll import the modules and setup the notebook. Then we'll define functions for each step - tokenizing, counting tokens, splitting, translating, and reconstructing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5231a58-0458-473d-a802-4f5f0caafe94",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db365a-1cf2-4936-ae1f-131f1894a7fe",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ff99b0-5fb7-407f-9a2a-be72da68098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_language = \"english\"\n",
    "output_language = \"french\"\n",
    "input_path = \"data/input.txt\"\n",
    "format = \"markdown\" # any special formatting considerations (e.g. .arb file, markdown, json, plain text, or multiple)\n",
    "split_string = \"\\n\\n\" # the split string used to segment the chunks within the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c2c0fa-a4be-4ab7-89cc-0e9a692e0660",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f201397c-c5e3-4229-934a-57b640b7d511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.5.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2